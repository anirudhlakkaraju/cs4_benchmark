# CS4: Evaluating LLM Creativity in Story Generation

## Abstract

Evaluating the creativity of large language models (LLMs) in story writing is challenging since generated stories may resemble existing narratives in the models' training data. To address this, we introduce **CS4**, a benchmark dataset with prompts of varying specificity. By increasing prompt constraints, we prevent models from reproducing known stories, indirectly assessing their creativity.

Our experiments on models like **LLaMA**, **Gemma**, and **Mistral** show the difficulty LLMs face in balancing constraint satisfaction and narrative coherence, especially with highly specific prompts. We also demonstrate that **Learning from Human Feedback (LHF)**, tested with **OLMo**, improves story selection but has limited impact on generating genuinely creative stories.

## Table of Contents
- [Project Overview](#project-overview)
- [Installation](#installation)
- [Usage](#usage)
- [Datasets](#datasets)
- [Evaluation Scripts](#evaluation-scripts)
- [Results](#results)
- [License](#license)

## Project Overview

This repository contains the code and data associated with the **CS4** benchmark, designed to evaluate the creativity of large language models (LLMs) under various levels of constraint specificity. The benchmark allows us to investigate how LLMs balance **creativity**, **constraint satisfaction**, and **coherence** in story generation tasks. Additionally, the code includes evaluation scripts for the analysis of multiple LLMs' performance on CS4.

### Key Models Evaluated:
- **LLaMA**
- **Gemma**
- **Mistral**
- **OLMo** (with insights into the impact of Learning from Human Feedback)

### Key Components:
1. **CS4 Dataset**: Contains prompts with varying degrees of specificity to test the creativity of LLMs.
2. **Evaluation Scripts**: To measure constraint satisfaction, coherence, and perplexity of the generated stories.

## Installation

1. Clone this repository:
    ```bash
    git clone https://github.com/your-repo/cs4-evaluation.git
    cd cs4-evaluation
    ```

2. Set up a virtual environment and install the dependencies:
    ```bash
    python3 -m venv env
    source env/bin/activate
    pip install -r requirements.txt
    ```

3. Ensure you have the necessary API keys and environment variables set up for models requiring external APIs, such as OpenAI's API.

## Usage

### Running the Evaluation

You can run all evaluation scripts in one step using the `run_all_evals.py` script. This script requires specific input file paths for each evaluation module. Below is an example command:

```bash
python run_all_evals.py \
    --input_dir /path/to/input/files \
    --output_dir /path/to/output/files \
    --coherence_csv /path/to/coherence.csv \
    --file1 /path/to/file1.csv \
    --file2 /path/to/file2.csv \
    --file3 /path/to/file3.csv \
    --quc_input_json /path/to/quc_input.json \
    --story_quality_input /path/to/story_quality.csv \
    --api_key YOUR_OPENAI_API_KEY
```

This command will execute all evaluation steps, saving results and plots in the specified `output_dir`.

### Individual Scripts

Each evaluation script can also be run independently by providing the necessary arguments. For example, to evaluate **constraint satisfaction**:

```bash
python constraint_satisfaction.py \
    --input_path /path/to/input.csv \
    --output_path /path/to/output.csv
```

## Datasets

### CS4 Benchmark Dataset

The **CS4 dataset** is designed to evaluate LLM creativity by introducing prompts with varying levels of specificity:
- **Low-Specificity Prompts**: Fewer constraints, allowing for more creative freedom.
- **High-Specificity Prompts**: Many constraints, forcing the model to produce more structured outputs.

The dataset includes multiple stories generated by LLaMA, Gemma, Mistral, and OLMo models, all with different levels of instruction complexity.

## Evaluation Scripts

### Key Metrics:
1. **Constraint Satisfaction**: Measures how well the generated story adheres to the prompt's constraints.
2. **Coherence**: Evaluates the overall coherence of the generated narrative.
3. **Diversity**: Calculates n-gram diversity to assess story originality.
4. **Perplexity**: Measures the predictability and fluency of the generated text.
5. **QUC and RCS**: Metrics specifically designed for this benchmark to evaluate creativity under constraints.

The key scripts for evaluation include:
- `coherence_vs_constraint_graph.py`: Generates graphs comparing coherence vs. constraint satisfaction.
- `constraint_satisfaction.py`: Evaluates how well generated stories satisfy constraints.
- `diversity_calculation.py`: Calculates diversity in generated stories.
- `perplexity_graph_generation.py`: Plots perplexity against the number of constraints.
- `quc_and_rcs.py`: Computes and plots QUC and RCS scores.

## Results

In our experiments:
- **LLaMA**, **Gemma**, and **Mistral** show distinct performance across varying levels of constraint specificity.
- Models struggle with maintaining creativity when the prompt becomes highly specific.
- **OLMo** demonstrates improved story selection via Learning from Human Feedback (LHF), but it struggles to generate unseen creative stories, even with LHF.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.

---

For more information on the dataset and experiments, please refer to the accompanying research paper.